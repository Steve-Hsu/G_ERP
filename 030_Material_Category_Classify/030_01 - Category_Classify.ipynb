{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Category_Classify\n",
    "## Goal of the function\n",
    "* The dataSource of the func is M-List\n",
    "* Classify the material in the M-List, and fill the new colum \"Category\" with string such as \"Fabric\", \"Zipper\", \"Button\" etc.\n",
    "* Save the result as a Categoried_M_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# File system\n",
    "from os import walk\n",
    "from os.path import join\n",
    "\n",
    "# Switch area\n",
    "CATEGORY_LIST =['fabric', 'insulation', 'interfacing', 'zipper', 'label']\n",
    "\n",
    "# SUBJECT = 'label'\n",
    "VOCAB_SIZE = 900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM_DIR = 'DataSource/M-List'\n",
    "TO_DIR = 'result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  M_List = pd.read_csv(FROM_DIR + '/AB18MJ1_032_ BOM_032 BOM_material_list.csv', encoding = 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* df.insert(1, 'Category', '', True)\n",
    "\n",
    "* df.at[0,'Category'] = 'Steve'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "* We need to convert the M_list(Type in DF)through several process to Full matrix, so we can classify the M_List with the trained_data we made before.\n",
    "## Process\n",
    "### delete_col()\n",
    "### stemmered_nltk_convert()\n",
    "### turn_series()\n",
    "### make_sparse_matrix()\n",
    "### make_full_feature()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## delete_col()\n",
    "* Delete the none columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_col(m_list):\n",
    "    for col in m_list:\n",
    "        if m_list[col].count() == 0:\n",
    "            m_list = m_list.drop(col, axis = 1)\n",
    "        \n",
    "    return m_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stemmered_nltk_convert() \n",
    "* Nltk stemmered Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmered_nltk_convert(col_of_df):\n",
    "    '''\n",
    "    Parameter of this function is a column of a dataFrame.\n",
    "    \n",
    "    '''\n",
    "    # difine Stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Difine Stemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    # converts to lower case and splits up the words\n",
    "    words = word_tokenize(col_of_df)\n",
    "    filtered_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Removes the stop words and punctuation\n",
    "        # if word is not in the stop_words list and is not a alpha.\n",
    "        if word not in stop_words and word.isalpha():\n",
    "            filtered_words.append(stemmer.stem(word))\n",
    "            \n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## turn_series()\n",
    "#### Walk through bom\n",
    "* Parse a xlsm of bom, turn the row into a cell, all the cells will form a col.\n",
    "* Put the cell to the classify function\n",
    "* Return the index of row that is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_series(bom):\n",
    "    '''\n",
    "    \n",
    "    The parameter of the func is a dataFrame\n",
    "    \n",
    "    '''\n",
    "    database = []\n",
    "    \n",
    "    for row in bom.index:\n",
    "        row_str = str()\n",
    "        for col in bom:\n",
    "            row_str = row_str + ', ' + str(bom.at[row, col])\n",
    "        database.append(row_str)\n",
    "    \n",
    "    col = pd.Series(database)\n",
    "#     index_list = classify_series(col)\n",
    "    \n",
    "    return col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make_sparse_matrix()\n",
    "### Sparse Matrix Function\n",
    "* Create a sparse Matrix for the data we want to predict\n",
    "* The difference of this function in comparition with Classification Model for Train data, is this function don't need CATEGORY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sparse_matrix(df, vocabulary):\n",
    "    \"\"\"\n",
    "    Param1:\n",
    "    The data we want to sparse, which must be in format of DataFrame.\n",
    "    \n",
    "    Param2:\n",
    "    The vocabulary, it is generated when we training datas.\n",
    "    \n",
    "    Returns a sparse matrix as dataframe\n",
    "    \"\"\"\n",
    " \n",
    "    indexed_words = pd.Index(vocabulary.VOCAB_WORD)\n",
    "    nr_rows = df.shape[0]\n",
    "    nr_cols = df.shape[1]\n",
    "    word_set = set(indexed_words)\n",
    "    dict_list = []\n",
    "    \n",
    "    for i in range(nr_rows):\n",
    "        for j in range(nr_cols):\n",
    "            \n",
    "            word = df.iat[i, j]\n",
    "            if word in word_set:\n",
    "                doc_id = df.index[i]\n",
    "                word_id = indexed_words.get_loc(word)\n",
    "                \n",
    "                item = {'MATERIAL_ID': doc_id,\n",
    "                       'OCCURENCE': 1, 'WORD_ID': word_id}\n",
    "                \n",
    "                dict_list.append(item)\n",
    "                \n",
    "    return pd.DataFrame(dict_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make_full_feature()\n",
    "### Full Matrix\n",
    "* Since we want to predict the data, so we create the Full Feature directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_full_feature(sparse_matrix, nr_words, doc_idx = 0, word_idx = 1, freq_idx = 2):\n",
    "    column_names = ['MATERIAL_ID'] + list(range(0, VOCAB_SIZE))\n",
    "    doc_id_names = np.unique(sparse_matrix[:,0])\n",
    "    full_matrix = pd.DataFrame(index = doc_id_names, columns = column_names)\n",
    "    full_matrix.fillna(value=0, inplace=True)\n",
    "    \n",
    "    for i in range(sparse_matrix.shape[0]):\n",
    "        doc_nr = sparse_matrix[i][doc_idx]\n",
    "        word_id = sparse_matrix[i][word_idx]\n",
    "        occurrence = sparse_matrix[i][freq_idx]\n",
    "        \n",
    "        full_matrix.at[doc_nr, 'MATERIAL_ID'] = doc_nr\n",
    "        full_matrix.at[doc_nr, word_id] = occurrence\n",
    "        \n",
    "    full_matrix.set_index('MATERIAL_ID', inplace = True)\n",
    "    return full_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Funciton\n",
    "## Material_Classifor\n",
    "* Classifor the materials in the M-List, \n",
    "* Mateiral is in row direction in a M-List.\n",
    "* Return a list of index of row that classified as \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def material_classifor(bom, SUBJECT):\n",
    "    VOCAB  = 'DataSource/Trained Data/' + SUBJECT + '_vocabulary.csv'\n",
    "    TRAIN_DATA_1 = 'DataSource/Trained Data/' + SUBJECT + '_prob_tokens_ctg_1_in_train_data'\n",
    "    TRAIN_DATA_0 = 'DataSource/Trained Data/' + SUBJECT + '_prob_tokens_ctg_0_in_train_data'\n",
    "    TRAIN_DATA_ALL = 'DataSource/Trained Data/' + SUBJECT + '_prob_tokens_all_in_train_data'\n",
    "    PROB_1_TRAIN_DATA = 'DataSource/Trained Data/' + SUBJECT + '_prob_ctg_1_in_train_data'\n",
    "    '''\n",
    "    Param_1\n",
    "    <bom>\n",
    "    DataFrame, by reading M-List in csv format.\n",
    "    \n",
    "    Param_2\n",
    "    <SUBJECT>\n",
    "    It defines what to analyze, for example, of the SUBJECT == 'fabric', \n",
    "    then the func will use the trained_data set of fabric to analyze the documents.\n",
    "    \n",
    "    Local_var_1\n",
    "    <VOCAB>\n",
    "    The path of the vocabulary\n",
    "    Token list with WORD_ID\n",
    "    \n",
    "    Local_var_2\n",
    "    <TRAIN_DATA_1>\n",
    "    The trained data of catagory True\n",
    "    Probabilitie of each token in category True\n",
    "    \n",
    "    Local_var_3\n",
    "    <TRAIN_DATA_0>\n",
    "    The trained data of category False\n",
    "    Probabilitie of each token in category False\n",
    "    \n",
    "    Local_var_4\n",
    "    <TRAIN_DATA_ALL>\n",
    "    The trained data of category both.\n",
    "    Probabilitie of each token in all documents\n",
    "    \n",
    "    Local_var_5\n",
    "    <PROB_1_TRAIN_DATA>\n",
    "    The percentage of documents in catagory True in all documents.\n",
    "    Number of documents in catagory True / number of all documents\n",
    "    '''\n",
    "    # read the vocabulary\n",
    "    vocab = pd.read_csv(VOCAB, index_col = 0)\n",
    "    # read the trained_datas\n",
    "    train_data_1 = np.loadtxt(TRAIN_DATA_1)\n",
    "    train_data_0 = np.loadtxt(TRAIN_DATA_0)\n",
    "    train_data_all = np.loadtxt(TRAIN_DATA_ALL)\n",
    "    prob_ctg_1 = pd.read_csv(PROB_1_TRAIN_DATA, index_col = 0)\n",
    "    prob_ctg_1_train_data = prob_ctg_1.loc[0, 'prob_ctg_1_train_set']\n",
    "    \n",
    "    \n",
    "    # Delete useless cols\n",
    "    col_deleted_bom = delete_col(bom)\n",
    "    \n",
    "    # Series\n",
    "    # Parse the bom, make each col getting together to be 1 col\n",
    "    new_bom = turn_series(col_deleted_bom)\n",
    "    \n",
    "    # nltk_convert\n",
    "    stemmed_bom = new_bom.apply(stemmered_nltk_convert)\n",
    "    \n",
    "    # Convert the stemmed series into df\n",
    "    # 1 token get 1 cell\n",
    "    word_col_df = pd.DataFrame.from_records(stemmed_bom.tolist())\n",
    "    \n",
    "    # Sparse Matrix\n",
    "    # Create a sparse Matrix for the data we want to predict\n",
    "    # The difference of this function in comparition with Classification Model for Train data, is this function don't need CATEGORY.\n",
    "    sparse_predict_df = make_sparse_matrix(word_col_df, vocab)\n",
    "    # Grouped by MATERIAL_ID\n",
    "    sparse_predict_df_grouped = sparse_predict_df.groupby(['MATERIAL_ID', 'WORD_ID']).sum()\n",
    "    # Reset it index\n",
    "    sparse_predict_df_grouped = sparse_predict_df_grouped.reset_index()\n",
    "    # Convert it into numpy array.\n",
    "    sparse_predict_data = sparse_predict_df_grouped.to_numpy()\n",
    "    \n",
    "    #Full Matrix\n",
    "    predict_full_feature = make_full_feature(sparse_predict_data, vocab.shape[0])\n",
    "    \n",
    "    #Joint probability in log format\n",
    "    joint_log_ctg_1 = predict_full_feature.dot(np.log(train_data_1) - np.log(train_data_all)) + np.log(prob_ctg_1_train_data)\n",
    "    joint_log_ctg_0 = predict_full_feature.dot(np.log(train_data_0)-np.log(train_data_all))+np.log(1 - prob_ctg_1_train_data)\n",
    "    # Prediction\n",
    "    prediction_log = joint_log_ctg_1 > joint_log_ctg_0\n",
    "    \n",
    "    # Get the index of the row that predicted as material in the bom\n",
    "    row_list = prediction_log[prediction_log == True].index\n",
    "\n",
    "    #2020/03/11\n",
    "    # The difference with the func \"M-List_generator\" is that the func only return a list of index classified as True.\n",
    "    # Later I may optimize the func \"M-List_generator\" same as this func, so the two func can use same code as this func.\n",
    "    # Let the different part be done outside the func.\n",
    "#     # Get the material from the original bom by the index in row_list\n",
    "#     material_list = bom.loc[row_list,:]\n",
    "    \n",
    "    return row_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Funciton\n",
    "## Loop through materials designated as category.\n",
    "* Analyze a M-List with several sets of train-data and vocabulary. \n",
    "* Each set of trainned-data and vocabulary represents 1 category of material, such as fabric, zipper, label.\n",
    "* This func will feed the func \"material_classifor\" each set of trainned-data and vocabulary by order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 'DataSource/M-List/AB18MJ1_032_ BOM_032 BOM_material_list.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_out_col_category(M_list, Style_name, LIST = CATEGORY_LIST) :\n",
    "    '''\n",
    "    Param_1\n",
    "    <M_list>\n",
    "    M_list in CSV format\n",
    "    The func will turn it into a DataFrame while calculating.\n",
    "    \n",
    "    Param_2\n",
    "    The name of the file.\n",
    "\n",
    "    Param_3\n",
    "    An array in type List.\n",
    "    The content is the categories of material.\n",
    "    That decides which train_set data to be used.\n",
    "    '''\n",
    "    bom = pd.read_csv(M_list, index_col = None, encoding = 'ISO-8859-1')\n",
    "    bom.insert(1, 'CATEGORY', 'other', True)\n",
    "    \n",
    "    for cate in LIST:\n",
    "        ROW_LIST = material_classifor(bom, cate)\n",
    "        bom.at[ROW_LIST, 'CATEGORY'] = cate\n",
    "    \n",
    "    bom.to_csv('result/' + Style_name + '_CATE_classified_M-List.csv')\n",
    "    return bom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walk through a dir \n",
    "* Check each xlsx in a dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_list_cate_classify_convertor(FROM_DIR, TO_DIR):\n",
    "    converted_csv_num = 0\n",
    "    for root, dirnames, filenames in walk(FROM_DIR):\n",
    "        # walk through each xlsx file\n",
    "        for file_name in filenames:\n",
    "             # get the path of the file\n",
    "            # Appoint the method only work with .xlsx file.\n",
    "            if file_name.endswith('.csv') :\n",
    "                converted_csv_num = converted_csv_num + 1\n",
    "                filepath = join(root, file_name)\n",
    "                # Custom function\n",
    "                fill_out_col_category(filepath, file_name[0:-18])\n",
    "    print('Converted ', converted_csv_num, ' files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted  2  files\n",
      "CPU times: user 3.57 s, sys: 41.8 ms, total: 3.62 s\n",
      "Wall time: 2.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "m_list_cate_classify_convertor(FROM_DIR, TO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
