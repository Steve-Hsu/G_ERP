{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column_Classify\n",
    "## Goal of the function\n",
    "### This func is basically an adapted copy of func \"Category_Classify\", that from last step.\n",
    "## Basic workflow\n",
    "* The dataSource of the func is Categoried_M_List\n",
    "* Classify each column in the Categoried_M_List, and fill the fitile for each column with string such as \"Item\", \"Description\", \"Position\" etc.\n",
    "* Save the result as a Col_classified_M_List\n",
    "\n",
    "## Problems\n",
    "* 2020/04/02 : Stucked in the \"Core Function\". \n",
    "* The fucn can classify in the direction of column, but when the COL_LIST added to much or other key word like \"insulation\", error will occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# File system\n",
    "from os import walk\n",
    "from os.path import join\n",
    "\n",
    "# Switch area\n",
    "COL_LIST =['item', 'description', 'color_way', 'position', 'spec', 'fabric']\n",
    "# COL_LIST =['item', 'description', 'spec', 'position', 'color_way', 'insulation']\n",
    "# COL_LIST =['fabric']\n",
    "# COL_LIST =['color_way']\n",
    "\n",
    "VOCAB_SIZE = 900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM_DIR = 'DataSource/Categoried_M-List'\n",
    "TO_DIR = 'result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test \n",
    "DATA = FROM_DIR + '/AB18MJ1_032_ BOM_032 BOM_CATE_classified_M-List.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "* We need to convert the M-List(Type in DF)through several process to Full matrix, so we can classify the M-List with the trained_data we made before.\n",
    "## Process\n",
    "### delete_col()\n",
    "### turn_series()\n",
    "### stemmered_nltk_convert()\n",
    "### make_sparse_matrix()\n",
    "### make_full_feature()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## delete_col()\n",
    "* Delete the none columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_col(m_list):\n",
    "    for col in m_list:\n",
    "        if m_list[col].count() == 0:\n",
    "            m_list = m_list.drop(col, axis = 1)\n",
    "    return m_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test\n",
    "# bom = pd.read_csv(DATA, index_col = None, encoding = 'ISO-8859-1')\n",
    "# M_List = delete_col(bom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## turn_series()\n",
    "#### Walk through bom\n",
    "* Parse a xlsm of bom, turn the columns into cells, all the cells will form a col.\n",
    "* Put the cell to the classify function\n",
    "* Return the index of column that is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_series(bom):\n",
    "    '''\n",
    "    bom: DataFrame, \n",
    "    \n",
    "    '''\n",
    "    database = []\n",
    "    \n",
    "    for col in bom:\n",
    "        col_str = str()\n",
    "        for row in bom.index:\n",
    "            col_str = col_str + ', ' + str(bom.at[row, col])\n",
    "        database.append(col_str)\n",
    "    \n",
    "    col = pd.Series(database)\n",
    "#     index_list = classify_series(col)\n",
    "    \n",
    "    return col#### Below is the original function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is the original function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def turn_series(bom):\n",
    "    '''\n",
    "    \n",
    "    The parameter of the func is a dataFrame\n",
    "    \n",
    "    '''\n",
    "    database = []\n",
    "    \n",
    "    for row in bom.index:\n",
    "        row_str = str()\n",
    "        for col in bom:\n",
    "            row_str = row_str + ', ' + str(bom.at[row, col])\n",
    "        database.append(row_str)\n",
    "    \n",
    "    col = pd.Series(database)\n",
    "*     index_list = classify_series(col)\n",
    "    \n",
    "    return col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stemmered_nltk_convert() \n",
    "* Nltk stemmered Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmered_nltk_convert(col_of_df):\n",
    "    '''\n",
    "    Parameter of this function is a column of a dataFrame.\n",
    "    \n",
    "    '''\n",
    "    # difine Stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Difine Stemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    # converts to lower case and splits up the words\n",
    "    words = word_tokenize(col_of_df)\n",
    "    filtered_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Removes the stop words and punctuation\n",
    "        # if word is not in the stop_words list and is not a alpha.\n",
    "        if word not in stop_words and word.isalpha():\n",
    "            filtered_words.append(stemmer.stem(word))\n",
    "            \n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# new_List = turn_series(M_List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "# new_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# stemmed_List = new_List.apply(stemmered_nltk_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# stemmed_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# word_col_df = pd.DataFrame.from_records(stemmed_List.tolist())\n",
    "# word_col_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make_sparse_matrix()\n",
    "### Sparse Matrix Function\n",
    "* Create a sparse Matrix for the data we want to predict\n",
    "* The difference of this function in comparition with Classification Model for Train data, is this function don't need CATEGORY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sparse_matrix(df, vocabulary):\n",
    "    \"\"\"\n",
    "    Param1:\n",
    "    The data we want to sparse, which must be in format of DataFrame.\n",
    "    \n",
    "    Param2:\n",
    "    The vocabulary, it is generated when we training datas.\n",
    "    \n",
    "    Returns a sparse matrix as dataframe\n",
    "    \"\"\"\n",
    " \n",
    "    indexed_words = pd.Index(vocabulary.VOCAB_WORD)\n",
    "    nr_rows = df.shape[0]\n",
    "    nr_cols = df.shape[1]\n",
    "    word_set = set(indexed_words)\n",
    "    dict_list = []\n",
    "    \n",
    "    for i in range(nr_rows):\n",
    "        for j in range(nr_cols):\n",
    "            \n",
    "            word = df.iat[i, j]\n",
    "            if word in word_set:\n",
    "                doc_id = df.index[i]\n",
    "                word_id = indexed_words.get_loc(word)\n",
    "                \n",
    "                item = {'MATERIAL_ID': doc_id,\n",
    "                       'OCCURENCE': 1, 'WORD_ID': word_id}\n",
    "                \n",
    "                dict_list.append(item)\n",
    "                \n",
    "    return pd.DataFrame(dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB  = 'DataSource/Trained Data/fabric_vocabulary.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pd.read_csv(VOCAB, index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_col_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-4b18b258da4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msparse_predict_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_sparse_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_col_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msparse_predict_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_col_df' is not defined"
     ]
    }
   ],
   "source": [
    "sparse_predict_df = make_sparse_matrix(word_col_df, vocab)\n",
    "sparse_predict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_predict_df_grouped = sparse_predict_df.groupby(['MATERIAL_ID', 'WORD_ID']).sum()\n",
    "sparse_predict_df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sparse_predict_df_grouped = sparse_predict_df_grouped.reset_index()\n",
    "sparse_predict_df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_predict_data = sparse_predict_df_grouped.to_numpy()\n",
    "sparse_predict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make_full_feature()\n",
    "### Full Matrix\n",
    "* Since we want to predict the data, so we create the Full Feature directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_full_feature(sparse_matrix, nr_words, doc_idx = 0, word_idx = 1, freq_idx = 2):\n",
    "    column_names = ['MATERIAL_ID'] + list(range(0, VOCAB_SIZE))\n",
    "    doc_id_names = np.unique(sparse_matrix[:,0])\n",
    "    full_matrix = pd.DataFrame(index = doc_id_names, columns = column_names)\n",
    "    full_matrix.fillna(value=0, inplace=True)\n",
    "    \n",
    "    for i in range(sparse_matrix.shape[0]):\n",
    "        doc_nr = sparse_matrix[i][doc_idx]\n",
    "        word_id = sparse_matrix[i][word_idx]\n",
    "        occurrence = sparse_matrix[i][freq_idx]\n",
    "        \n",
    "        full_matrix.at[doc_nr, 'MATERIAL_ID'] = doc_nr\n",
    "        full_matrix.at[doc_nr, word_id] = occurrence\n",
    "        \n",
    "    full_matrix.set_index('MATERIAL_ID', inplace = True)\n",
    "    return full_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_full_feature = make_full_feature(sparse_predict_data, vocab.shape[0])\n",
    "predict_full_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_1 = 'DataSource/Trained Data/fabric_prob_tokens_ctg_1_in_train_data'\n",
    "TRAIN_DATA_0 = 'DataSource/Trained Data/fabric_prob_tokens_ctg_0_in_train_data'\n",
    "TRAIN_DATA_ALL = 'DataSource/Trained Data/fabric_prob_tokens_all_in_train_data'\n",
    "PROB_1_TRAIN_DATA = 'DataSource/Trained Data/fabric_prob_ctg_1_in_train_data'\n",
    "train_data_1 = np.loadtxt(TRAIN_DATA_1)\n",
    "train_data_0 = np.loadtxt(TRAIN_DATA_0)\n",
    "train_data_all = np.loadtxt(TRAIN_DATA_ALL)\n",
    "prob_ctg_1 = pd.read_csv(PROB_1_TRAIN_DATA, index_col = 0)\n",
    "prob_ctg_1_train_data = prob_ctg_1.loc[0, 'prob_ctg_1_train_set']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "joint_log_ctg_1 = predict_full_feature.dot(np.log(train_data_1) - np.log(train_data_all)) + np.log(prob_ctg_1_train_data)\n",
    "joint_log_ctg_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "joint_log_ctg_0 = predict_full_feature.dot(np.log(train_data_0)-np.log(train_data_all))+np.log(1 - prob_ctg_1_train_data)\n",
    "joint_log_ctg_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_log = joint_log_ctg_1 > joint_log_ctg_0\n",
    "prediction_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Funciton\n",
    "## Material_Classifor\n",
    "* Classifor the materials in the M-List, \n",
    "* Mateiral is in row direction in a M-List.\n",
    "* Return a list of index of row that classified as \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def material_classifor(bom, SUBJECT):\n",
    "    VOCAB  = 'DataSource/Trained Data/' + SUBJECT + '_vocabulary.csv'\n",
    "    TRAIN_DATA_1 = 'DataSource/Trained Data/' + SUBJECT + '_prob_tokens_ctg_1_in_train_data'\n",
    "    TRAIN_DATA_0 = 'DataSource/Trained Data/' + SUBJECT + '_prob_tokens_ctg_0_in_train_data'\n",
    "    TRAIN_DATA_ALL = 'DataSource/Trained Data/' + SUBJECT + '_prob_tokens_all_in_train_data'\n",
    "    PROB_1_TRAIN_DATA = 'DataSource/Trained Data/' + SUBJECT + '_prob_ctg_1_in_train_data'\n",
    "    '''\n",
    "    Param_1\n",
    "    bom: String,\n",
    "    The path of a DataFrame, \n",
    "    by reading M-List in csv format.\n",
    "    \n",
    "    Param_2\n",
    "    SUBJECT: String, \n",
    "    It defines what to analyze, for example, of the SUBJECT == 'fabric', \n",
    "    then the func will use the trained_data set of fabric to analyze the documents.\n",
    "    \n",
    "    Local_var_1\n",
    "    VOCAB: String, \n",
    "    The path of the vocabulary\n",
    "    Token list with WORD_ID\n",
    "    \n",
    "    Local_var_2\n",
    "    TRAIN_DATA_1: String, \n",
    "    The path of trained data\n",
    "    The trained data of catagory True\n",
    "    Probabilitie of each token in category True\n",
    "    \n",
    "    Local_var_3\n",
    "    TRAIN_DATA_0: String, \n",
    "    The path of trained data\n",
    "    The trained data of category False\n",
    "    Probabilitie of each token in category False\n",
    "    \n",
    "    Local_var_4\n",
    "    TRAIN_DATA_ALL: String\n",
    "    The path of trained data\n",
    "    The trained data of category both.\n",
    "    Probabilitie of each token in all documents\n",
    "    \n",
    "    Local_var_5\n",
    "    PROB_1_TRAIN_DATA: String\n",
    "    The path of trained data\n",
    "    The percentage of documents in catagory True in all documents.\n",
    "    Number of documents in catagory True / number of all documents\n",
    "    '''\n",
    "    # read the vocabulary\n",
    "    vocab = pd.read_csv(VOCAB, index_col = 0)\n",
    "    # read the trained_datas\n",
    "    train_data_1 = np.loadtxt(TRAIN_DATA_1)\n",
    "    train_data_0 = np.loadtxt(TRAIN_DATA_0)\n",
    "    train_data_all = np.loadtxt(TRAIN_DATA_ALL)\n",
    "    prob_ctg_1 = pd.read_csv(PROB_1_TRAIN_DATA, index_col = 0)\n",
    "    prob_ctg_1_train_data = prob_ctg_1.loc[0, 'prob_ctg_1_train_set']\n",
    "    \n",
    "    \n",
    "    # Delete useless cols\n",
    "    col_deleted_bom = delete_col(bom)\n",
    "    \n",
    "    # Series\n",
    "    # Parse the bom, make each col getting together to be 1 col\n",
    "    new_bom = turn_series(col_deleted_bom)\n",
    "    \n",
    "    # nltk_convert\n",
    "    stemmed_bom = new_bom.apply(stemmered_nltk_convert)\n",
    "    \n",
    "    # Convert the stemmed series into df\n",
    "    # 1 token get 1 cell\n",
    "    word_col_df = pd.DataFrame.from_records(stemmed_bom.tolist())\n",
    "    \n",
    "    # Sparse Matrix\n",
    "    # Create a sparse Matrix for the data we want to predict\n",
    "    # The difference of this function in comparition with Classification Model for Train data, is this function don't need CATEGORY.\n",
    "    sparse_predict_df = make_sparse_matrix(word_col_df, vocab)\n",
    "    # Grouped by MATERIAL_ID\n",
    "    sparse_predict_df_grouped = sparse_predict_df.groupby(['MATERIAL_ID', 'WORD_ID']).sum()\n",
    "    # Reset it index\n",
    "    sparse_predict_df_grouped = sparse_predict_df_grouped.reset_index()\n",
    "    # Convert it into numpy array.\n",
    "    sparse_predict_data = sparse_predict_df_grouped.to_numpy()\n",
    "    \n",
    "    #Full Matrix\n",
    "    predict_full_feature = make_full_feature(sparse_predict_data, vocab.shape[0])\n",
    "    \n",
    "    #Joint probability in log format\n",
    "    joint_log_ctg_1 = predict_full_feature.dot(np.log(train_data_1) - np.log(train_data_all)) + np.log(prob_ctg_1_train_data)\n",
    "    joint_log_ctg_0 = predict_full_feature.dot(np.log(train_data_0)-np.log(train_data_all))+np.log(1 - prob_ctg_1_train_data)\n",
    "    # Prediction\n",
    "    prediction_log = joint_log_ctg_1 > joint_log_ctg_0\n",
    "    \n",
    "    # Get the index of the row that predicted as material in the bom\n",
    "    row_list = prediction_log[prediction_log == True].index\n",
    "    print(row_list)\n",
    "    #2020/03/11\n",
    "    # The difference with the func \"M-List_generator\" is that the func only return a list of index classified as True.\n",
    "    # Later I may optimize the func \"M-List_generator\" same as this func, so the two func can use same code as this func.\n",
    "    # Let the different part be done outside the func.\n",
    "#     # Get the material from the original bom by the index in row_list\n",
    "#     material_list = bom.loc[row_list,:]\n",
    "    \n",
    "    return row_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Funciton\n",
    "## Loop through materials designated as category.\n",
    "* Analyze a Categoried_M-List with several sets of train-data and vocabulary. \n",
    "* Each set of trainned-data and vocabulary represents 1 category of material, such as item, description, spec.\n",
    "* This func will feed the func \"material_classifor\" each set of trainned-data and vocabulary by order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 'DataSource/M-List/AB18MJ1_032_ BOM_032 BOM_material_list.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_out_col_category(M_list, Style_name, LIST = COL_LIST) :\n",
    "    '''\n",
    "    # Arguments\n",
    "        M_list: String,  \n",
    "            the path of M_list in CSV format. M_list in CSV format. The func will turn it into a DataFrame while calculating.\n",
    "            \n",
    "        Style_name: String,  \n",
    "            the name of the file.\n",
    "            \n",
    "        LIST: List,  \n",
    "            An array in type List. The content is the categories of material. That decides which train_set data to be used.\n",
    "    \n",
    "    # Returns\n",
    "        A M_List in csv fomat, filled out the name of columns, such as 'item', 'description', 'spec', etc.\n",
    "    '''\n",
    "    bom = pd.read_csv(M_list, index_col = None, encoding = 'ISO-8859-1')\n",
    "    #The code below may not using in the col classify.\n",
    "#     bom.insert(1, 'CATEGORY', 'other', True)\n",
    "\n",
    "    # Add a new row\n",
    "    new_bom = bom.append(pd.Series(name = 'TITLE'))\n",
    "    \n",
    "    new_bom.loc['TITLE', 'MATERIAL_ID'] = 'MATERIAL_ID'\n",
    "    new_bom.loc['TITLE', 'CATEGORY'] = 'CATEGORY'\n",
    "\n",
    "\n",
    "    for cate in LIST:\n",
    "        ROW_LIST = material_classifor(bom, cate)\n",
    "        print(ROW_LIST.values)\n",
    "        \n",
    "        # Loop through the ROW_LIST, change the cell in the row \"TITLE\" and in the col that with integer postion as the Int in ROW_LIST. \n",
    "        # notice, the method iat can only handle one col for one time, so here must use For Loop to get the job done.\n",
    "        # Since the new row \"TITLE\" is on the buttom of the df, the new_bom, so the integer position of the row is '-1'\n",
    "        for index in ROW_LIST:\n",
    "            new_bom.iat[-1, index] = cate\n",
    "            \n",
    "    #Set the row \"TITLE\" as the columns of the new_bom\n",
    "    new_bom.columns = new_bom.iloc[-1]\n",
    "    #Delete the last row, that must and should be the row \"TITLE\"\n",
    "    new_bom.drop(new_bom.index[-1], inplace = True)\n",
    "    \n",
    "    new_bom.to_csv('result/Col_Classify_M-List/' + Style_name + '_COL_classified_M-List.csv')\n",
    "    return new_bom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bom = pd.read_csv(DATA, index_col = None, encoding = 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walk through a dir \n",
    "* Check each xlsx in a dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_list_cate_classify_convertor(FROM_DIR, TO_DIR):\n",
    "    converted_csv_num = 0\n",
    "    for root, dirnames, filenames in walk(FROM_DIR):\n",
    "        # walk through each xlsx file\n",
    "        for file_name in filenames:\n",
    "             # get the path of the file\n",
    "            # Appoint the method only work with .xlsx file.\n",
    "            if file_name.endswith('.csv') :\n",
    "                converted_csv_num = converted_csv_num + 1\n",
    "                filepath = join(root, file_name)\n",
    "                # Custom function\n",
    "                fill_out_col_category(filepath, file_name[0:-18])\n",
    "    print('Converted ', converted_csv_num, ' files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill_out_col_category(FROM_DIR + '/081__classified_M-List.csv', 'Test_JSON') // Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "m_list_cate_classify_convertor(FROM_DIR, TO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
