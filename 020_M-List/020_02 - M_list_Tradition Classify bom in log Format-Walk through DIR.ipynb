{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'nltk' has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-30a1e7ba0cb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_shell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2478\u001b[0m \u001b[0;31m# Aliases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2479\u001b[0;31m \u001b[0m_downloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDownloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2480\u001b[0m \u001b[0mdownload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_downloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, server_index_url, download_dir)\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;31m# decide where we're going to save things to.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_download_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m     \u001b[0;31m# /////////////////////////////////////////////////////////////////\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mdefault_download_dir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1079\u001b[0m         \u001b[0;31m# Check if we have sufficient permissions to install in a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m         \u001b[0;31m# variety of system-wide locations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mnltkdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltkdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltkdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnltkdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'nltk' has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# File system\n",
    "from os import walk\n",
    "from os.path import join\n",
    "\n",
    "# Switch area\n",
    "SUBJECT = 'material_garment'\n",
    "VOCAB_SIZE = 900\n",
    "SHEET_NAME_LIST = ['bom', 'trims', 'shell', 'fabric', 'accessories', 'spec', '228184', '#334183', 'tabelle1']\n",
    "# FILE_NAME = 'Ride_18M20-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SUBJECT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-612987b0922e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mTO_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'result'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# BOM = 'DataSource/'+ FILE_NAME + '.xlsx'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mVOCAB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'DataSource/Trained Data/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mSUBJECT\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_vocabulary.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mTRAIN_DATA_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'DataSource/Trained Data/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mSUBJECT\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_prob_tokens_ctg_0_in_train_data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mTRAIN_DATA_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'DataSource/Trained Data/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mSUBJECT\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_prob_tokens_ctg_1_in_train_data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SUBJECT' is not defined"
     ]
    }
   ],
   "source": [
    "FROM_DIR = 'DataSource/Original_BOM'\n",
    "TO_DIR = 'result'\n",
    "# BOM = 'DataSource/'+ FILE_NAME + '.xlsx'\n",
    "VOCAB = 'DataSource/Trained Data/' + SUBJECT + '_vocabulary.csv'\n",
    "TRAIN_DATA_0 = 'DataSource/Trained Data/' + SUBJECT + '_prob_tokens_ctg_0_in_train_data'\n",
    "TRAIN_DATA_1 = 'DataSource/Trained Data/' + SUBJECT + '_prob_tokens_ctg_1_in_train_data'\n",
    "TRAIN_DATA_ALL =  'DataSource/Trained Data/' + SUBJECT + '_prob_tokens_all_in_train_data'\n",
    "PROB_1_TRAIN_DATA = 'DataSource/Trained Data/' + SUBJECT + '_prob_ctg_1_in_train_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## delete_col()\n",
    "* Delete the none columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_col(bom):\n",
    "    for col in bom:\n",
    "        if bom[col].count() == 0:\n",
    "            bom = bom.drop(col, axis = 1)\n",
    "        \n",
    "    return bom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stemmered_nltk_convert() \n",
    "* Nltk stemmered Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmered_nltk_convert(col_of_df):\n",
    "    '''\n",
    "    Parameter of this function is a column of a dataFrame.\n",
    "    \n",
    "    '''\n",
    "    # difine Stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Difine Stemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    # converts to lower case and splits up the words\n",
    "    words = word_tokenize(col_of_df)\n",
    "    filtered_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Removes the stop words and punctuation\n",
    "        # if word is not in the stop_words list and is not a alpha.\n",
    "        if word not in stop_words and word.isalpha():\n",
    "            filtered_words.append(stemmer.stem(word))\n",
    "            \n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## turn_series()\n",
    "#### Walk through bom\n",
    "* Parse a xlsm of bom, turn the row into a cell, all the cells will form a col.\n",
    "* Put the cell to the classify function\n",
    "* Return the index of row that is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_series(bom):\n",
    "    '''\n",
    "    \n",
    "    The parameter of the func is a dataFrame\n",
    "    \n",
    "    '''\n",
    "    database = []\n",
    "    \n",
    "    for row in bom.index:\n",
    "        row_str = str()\n",
    "        for col in bom:\n",
    "            row_str = row_str + ', ' + str(bom.at[row, col])\n",
    "        database.append(row_str)\n",
    "    \n",
    "    col = pd.Series(database)\n",
    "#     index_list = classify_series(col)\n",
    "    \n",
    "    return col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make_sparse_matrix()\n",
    "### Sparse Matrix Function\n",
    "* Create a sparse Matrix for the data we want to predict\n",
    "* The difference of this function in comparition with Classification Model for Train data, is this function don't need CATEGORY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sparse_matrix(df, vocabulary):\n",
    "    \"\"\"\n",
    "    Param1:\n",
    "    The data we want to sparse, which must be in format of DataFrame.\n",
    "    Param2:\n",
    "    The vocabulary, it is generated when we training datas.\n",
    "    \n",
    "    Returns a sparse matrix as dataframe\n",
    "    \"\"\"\n",
    " \n",
    "    indexed_words = pd.Index(vocabulary.VOCAB_WORD)\n",
    "    nr_rows = df.shape[0]\n",
    "    nr_cols = df.shape[1]\n",
    "    word_set = set(indexed_words)\n",
    "    dict_list = []\n",
    "    \n",
    "    for i in range(nr_rows):\n",
    "        for j in range(nr_cols):\n",
    "            \n",
    "            word = df.iat[i, j]\n",
    "            if word in word_set:\n",
    "                doc_id = df.index[i]\n",
    "                word_id = indexed_words.get_loc(word)\n",
    "                \n",
    "                item = {'MATERIAL_ID': doc_id,\n",
    "                       'OCCURENCE': 1, 'WORD_ID': word_id}\n",
    "                \n",
    "                dict_list.append(item)\n",
    "                \n",
    "    return pd.DataFrame(dict_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make_full_feature()\n",
    "### Full Matrix\n",
    "* Since we want to predict the data, so we create the Full Feature directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_full_feature(sparse_matrix, nr_words, doc_idx = 0, word_idx = 1, freq_idx = 2):\n",
    "    column_names = ['MATERIAL_ID'] + list(range(0, VOCAB_SIZE))\n",
    "    doc_id_names = np.unique(sparse_matrix[:,0])\n",
    "    full_matrix = pd.DataFrame(index = doc_id_names, columns = column_names)\n",
    "    full_matrix.fillna(value=0, inplace=True)\n",
    "    \n",
    "    for i in range(sparse_matrix.shape[0]):\n",
    "        doc_nr = sparse_matrix[i][doc_idx]\n",
    "        word_id = sparse_matrix[i][word_idx]\n",
    "        occurrence = sparse_matrix[i][freq_idx]\n",
    "        \n",
    "        full_matrix.at[doc_nr, 'MATERIAL_ID'] = doc_nr\n",
    "        full_matrix.at[doc_nr, word_id] = occurrence\n",
    "        \n",
    "    full_matrix.set_index('MATERIAL_ID', inplace = True)\n",
    "    return full_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Funciton\n",
    "## M-List_generator\n",
    "* Pick up material rows from a bom of xlsx and form it a dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def material_list_generator(bom, \n",
    "                       VOCAB = 'DataSource/Trained Data/' + SUBJECT + '_vocabulary.csv',\n",
    "                       TRAIN_DATA_1 = 'DataSource/Trained Data/' + SUBJECT + '_prob_tokens_ctg_1_in_train_data', \n",
    "                       TRAIN_DATA_0 = 'DataSource/Trained Data/' + SUBJECT + '_prob_tokens_ctg_0_in_train_data', \n",
    "                       TRAIN_DATA_ALL = 'DataSource/Trained Data/' + SUBJECT + '_prob_tokens_all_in_train_data', \n",
    "                       PROB_1_TRAIN_DATA = 'DataSource/Trained Data/' + SUBJECT + '_prob_ctg_1_in_train_data' ):\n",
    "    '''\n",
    "    Param_1\n",
    "    The dataframe of the excel\n",
    "    \n",
    "    Param_2\n",
    "    The path of the vocabulary\n",
    "    Token list with WORD_ID\n",
    "    \n",
    "    Param_3\n",
    "    The trained data of catagory True\n",
    "    Probabilitie of each token in category True\n",
    "    \n",
    "    Param_4\n",
    "    The trained data of category False\n",
    "    Probabilitie of each token in category False\n",
    "    \n",
    "    Param_5\n",
    "    The trained data of category both.\n",
    "    Probabilitie of each token in all documents\n",
    "    \n",
    "    Param_6\n",
    "    The percentage of documents in catagory True in all documents.\n",
    "    Number of documents in catagory True / number of all documents\n",
    "    '''\n",
    "    # read the vocabulary\n",
    "    vocab = pd.read_csv(VOCAB, index_col = 0)\n",
    "    # read the trained_datas\n",
    "    train_data_1 = np.loadtxt(TRAIN_DATA_1)\n",
    "    train_data_0 = np.loadtxt(TRAIN_DATA_0)\n",
    "    train_data_all = np.loadtxt(TRAIN_DATA_ALL)\n",
    "    prob_ctg_1 = pd.read_csv(PROB_1_TRAIN_DATA, index_col = 0)\n",
    "    prob_ctg_1_train_data = prob_ctg_1.loc[0, 'prob_ctg_1_train_set']\n",
    "    \n",
    "    \n",
    "    # Delete useless cols\n",
    "    col_deleted_bom = delete_col(bom)\n",
    "    \n",
    "    # Series\n",
    "    # Parse the bom, make each col getting together to be 1 col\n",
    "    new_bom = turn_series(col_deleted_bom)\n",
    "    \n",
    "    # nltk_convert\n",
    "    stemmed_bom = new_bom.apply(stemmered_nltk_convert)\n",
    "    \n",
    "    # Convert the stemmed series into df\n",
    "    # 1 token get 1 cell\n",
    "    word_col_df = pd.DataFrame.from_records(stemmed_bom.tolist())\n",
    "    \n",
    "    # Sparse Matrix\n",
    "    # Create a sparse Matrix for the data we want to predict\n",
    "    # The difference of this function in comparition with Classification Model for Train data, is this function don't need CATEGORY.\n",
    "    sparse_predict_df = make_sparse_matrix(word_col_df, vocab)\n",
    "    # Grouped by MATERIAL_ID\n",
    "    sparse_predict_df_grouped = sparse_predict_df.groupby(['MATERIAL_ID', 'WORD_ID']).sum()\n",
    "    # Reset it index\n",
    "    sparse_predict_df_grouped = sparse_predict_df_grouped.reset_index()\n",
    "    # Convert it into numpy array.\n",
    "    sparse_predict_data = sparse_predict_df_grouped.to_numpy()\n",
    "    \n",
    "    #Full Matrix\n",
    "    predict_full_feature = make_full_feature(sparse_predict_data, vocab.shape[0])\n",
    "    \n",
    "    #Joint probability in log format\n",
    "    joint_log_ctg_1 = predict_full_feature.dot(np.log(train_data_1) - np.log(train_data_all)) + np.log(prob_ctg_1_train_data)\n",
    "    joint_log_ctg_0 = predict_full_feature.dot(np.log(train_data_0)-np.log(train_data_all))+np.log(1 - prob_ctg_1_train_data)\n",
    "    # Prediction\n",
    "    prediction_log = joint_log_ctg_1 > joint_log_ctg_0\n",
    "    \n",
    "    #Get the index of the row that predicted as material in the bom\n",
    "    row_list = prediction_log[prediction_log == True].index\n",
    "    \n",
    "    # Get the material from the original bom by the index in row_list\n",
    "    material_list = bom.loc[row_list,:]\n",
    "    \n",
    "    return material_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walk through a workbook \n",
    "* Check each sheet in the workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sheet_checkor(BOM, Style_name, SHEET_NAME_LIST = SHEET_NAME_LIST):\n",
    "    xl = pd.ExcelFile(BOM)\n",
    "    \n",
    "    for sheet in xl.sheet_names:\n",
    "        name = str(sheet).lower()\n",
    "        if any(x in name for x in SHEET_NAME_LIST): \n",
    "            bom = xl.parse(sheet, index_col = None, header = None)\n",
    "            bom_list = material_list_generator(bom)\n",
    "            \n",
    "            # Delete empty columns\n",
    "            for col in bom_list:\n",
    "                if bom_list[col].count() == 0:\n",
    "                    bom_list = bom_list.drop(col, axis = 1)\n",
    "\n",
    "            bom_list.to_csv('result/' + Style_name + '_' + sheet + '_material_list.csv')\n",
    "            print(name)\n",
    "            \n",
    "    print('Job finished')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walk through a dir \n",
    "* Check each xlsx in a dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def material_list_convertor(FROM_DIR, TO_DIR):\n",
    "    converted_xlsm_num = 0\n",
    "    for root, dirnames, filenames in walk(FROM_DIR):\n",
    "        # walk through each xlsx file\n",
    "        for file_name in filenames:\n",
    "             # get the path of the file\n",
    "            # Appoint the method only work with .xlsx file.\n",
    "            if file_name.endswith('.xlsx') :\n",
    "                converted_xlsm_num = converted_xlsm_num + 1\n",
    "                filepath = join(root, file_name)\n",
    "                # Custom function\n",
    "                sheet_checkor(filepath, file_name[0:-5])\n",
    "    print('Converted ', converted_xlsm_num, ' files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "material_list_convertor(FROM_DIR, TO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
