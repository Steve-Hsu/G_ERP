{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# File system\n",
    "from os import walk\n",
    "from os.path import join\n",
    "\n",
    "# Switch area\n",
    "SUBJECT = 'material_garment'\n",
    "VOCAB_SIZE = 900\n",
    "SHEET_NAME_LIST = ['bom', 'trims', 'shell', 'fabric', 'accessories', 'spec', '228184', '#334183', 'tabelle1']\n",
    "# FILE_NAME = 'Ride_18M20-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM_DIR = 'DataSource/Original_BOM'\n",
    "TO_DIR = 'result'\n",
    "# BOM = 'DataSource/'+ FILE_NAME + '.xlsx'\n",
    "VOCAB = 'DataSource/' + SUBJECT + '_vocabulary.csv'\n",
    "TRAIN_DATA_0 = 'DataSource/Trained Data/' + SUBJECT + '_prob_tokens_ctg_0_in_train_data'\n",
    "TRAIN_DATA_1 = 'DataSource/Trained Data/' + SUBJECT + '_prob_tokens_ctg_1_in_train_data'\n",
    "TRAIN_DATA_ALL =  'DataSource/Trained Data/' + SUBJECT + '_prob_tokens_all_in_train_data'\n",
    "PROB_1_TRAIN_DATA = 'DataSource/Trained Data/' + SUBJECT + '_prob_ctg_1_in_train_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## delete_col()\n",
    "* Delete the none columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_col(bom):\n",
    "    for col in bom:\n",
    "        if bom[col].count() == 0:\n",
    "            bom = bom.drop(col, axis = 1)\n",
    "        \n",
    "    return bom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stemmered_nltk_convert() \n",
    "* Nltk stemmered Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmered_nltk_convert(col_of_df):\n",
    "    '''\n",
    "    Parameter of this function is a column of a dataFrame.\n",
    "    \n",
    "    '''\n",
    "    # difine Stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Difine Stemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    # converts to lower case and splits up the words\n",
    "    words = word_tokenize(col_of_df)\n",
    "    filtered_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Removes the stop words and punctuation\n",
    "        # if word is not in the stop_words list and is not a alpha.\n",
    "        if word not in stop_words and word.isalpha():\n",
    "            filtered_words.append(stemmer.stem(word))\n",
    "            \n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## turn_series()\n",
    "#### Walk through bom\n",
    "* Parse a xlsm of bom, turn the row into a cell, all the cells will form a col.\n",
    "* Put the cell to the classify function\n",
    "* Return the index of row that is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_series(bom):\n",
    "    '''\n",
    "    \n",
    "    The parameter of the func is a dataFrame\n",
    "    \n",
    "    '''\n",
    "    database = []\n",
    "    \n",
    "    for row in bom.index:\n",
    "        row_str = str()\n",
    "        for col in bom:\n",
    "            row_str = row_str + ', ' + str(bom.at[row, col])\n",
    "        database.append(row_str)\n",
    "    \n",
    "    col = pd.Series(database)\n",
    "#     index_list = classify_series(col)\n",
    "    \n",
    "    return col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make_sparse_matrix()\n",
    "### Sparse Matrix Function\n",
    "* Create a sparse Matrix for the data we want to predict\n",
    "* The difference of this function in comparition with Classification Model for Train data, is this function don't need CATEGORY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sparse_matrix(df, vocabulary):\n",
    "    \"\"\"\n",
    "    Param1:\n",
    "    The data we want to sparse, which must be in format of DataFrame.\n",
    "    Param2:\n",
    "    The vocabulary, it is generated when we training datas.\n",
    "    \n",
    "    Returns a sparse matrix as dataframe\n",
    "    \"\"\"\n",
    " \n",
    "    indexed_words = pd.Index(vocabulary.VOCAB_WORD)\n",
    "    nr_rows = df.shape[0]\n",
    "    nr_cols = df.shape[1]\n",
    "    word_set = set(indexed_words)\n",
    "    dict_list = []\n",
    "    \n",
    "    for i in range(nr_rows):\n",
    "        for j in range(nr_cols):\n",
    "            \n",
    "            word = df.iat[i, j]\n",
    "            if word in word_set:\n",
    "                doc_id = df.index[i]\n",
    "                word_id = indexed_words.get_loc(word)\n",
    "                \n",
    "                item = {'MATERIAL_ID': doc_id,\n",
    "                       'OCCURENCE': 1, 'WORD_ID': word_id}\n",
    "                \n",
    "                dict_list.append(item)\n",
    "                \n",
    "    return pd.DataFrame(dict_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make_full_feature()\n",
    "### Full Matrix\n",
    "* Since we want to predict the data, so we create the Full Feature directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_full_feature(sparse_matrix, nr_words, doc_idx = 0, word_idx = 1, freq_idx = 2):\n",
    "    column_names = ['MATERIAL_ID'] + list(range(0, VOCAB_SIZE))\n",
    "    doc_id_names = np.unique(sparse_matrix[:,0])\n",
    "    full_matrix = pd.DataFrame(index = doc_id_names, columns = column_names)\n",
    "    full_matrix.fillna(value=0, inplace=True)\n",
    "    \n",
    "    for i in range(sparse_matrix.shape[0]):\n",
    "        doc_nr = sparse_matrix[i][doc_idx]\n",
    "        word_id = sparse_matrix[i][word_idx]\n",
    "        occurrence = sparse_matrix[i][freq_idx]\n",
    "        \n",
    "        full_matrix.at[doc_nr, 'MATERIAL_ID'] = doc_nr\n",
    "        full_matrix.at[doc_nr, word_id] = occurrence\n",
    "        \n",
    "    full_matrix.set_index('MATERIAL_ID', inplace = True)\n",
    "    return full_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Funciton\n",
    "## M-List_generator\n",
    "* Pick up material rows from a bom of xlsx and form it a dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def material_list_generator(bom, \n",
    "                       VOCAB = 'DataSource/Trained Data/' + SUBJECT + '_vocabulary.csv',\n",
    "                       TRAIN_DATA_1 = 'DataSource/Trained Data/' + SUBJECT + '_prob_tokens_ctg_1_in_train_data', \n",
    "                       TRAIN_DATA_0 = 'DataSource/Trained Data/' + SUBJECT + '_prob_tokens_ctg_0_in_train_data', \n",
    "                       TRAIN_DATA_ALL = 'DataSource/Trained Data/' + SUBJECT + '_prob_tokens_all_in_train_data', \n",
    "                       PROB_1_TRAIN_DATA = 'DataSource/Trained Data/' + SUBJECT + '_prob_ctg_1_in_train_data' ):\n",
    "    '''\n",
    "    Param_1\n",
    "    The dataframe of the excel\n",
    "    \n",
    "    Param_2\n",
    "    The path of the vocabulary\n",
    "    Token list with WORD_ID\n",
    "    \n",
    "    Param_3\n",
    "    The trained data of catagory True\n",
    "    Probabilitie of each token in category True\n",
    "    \n",
    "    Param_4\n",
    "    The trained data of category False\n",
    "    Probabilitie of each token in category False\n",
    "    \n",
    "    Param_5\n",
    "    The trained data of category both.\n",
    "    Probabilitie of each token in all documents\n",
    "    \n",
    "    Param_6\n",
    "    The probability of documents in catagory True in all documents.\n",
    "    Number of documents in catagory True / number of all documents\n",
    "    '''\n",
    "    # read the vocabulary\n",
    "    vocab = pd.read_csv(VOCAB, index_col = 0)\n",
    "    # read the trained_datas\n",
    "    train_data_1 = np.loadtxt(TRAIN_DATA_1)\n",
    "    train_data_0 = np.loadtxt(TRAIN_DATA_0)\n",
    "    train_data_all = np.loadtxt(TRAIN_DATA_ALL)\n",
    "    prob_ctg_1 = pd.read_csv(PROB_1_TRAIN_DATA, index_col = 0)\n",
    "    prob_ctg_1_train_data = prob_ctg_1.loc[0, 'prob_ctg_1_train_set']\n",
    "    \n",
    "    \n",
    "    # Delete useless cols\n",
    "    col_deleted_bom = delete_col(bom)\n",
    "    \n",
    "    # Series\n",
    "    # Parse the bom, make each col getting together to be 1 col\n",
    "    new_bom = turn_series(col_deleted_bom)\n",
    "    \n",
    "    # nltk_convert\n",
    "    stemmed_bom = new_bom.apply(stemmered_nltk_convert)\n",
    "    \n",
    "    # Convert the stemmed series into df\n",
    "    # 1 token get 1 cell\n",
    "    word_col_df = pd.DataFrame.from_records(stemmed_bom.tolist())\n",
    "    \n",
    "    # Sparse Matrix\n",
    "    # Create a sparse Matrix for the data we want to predict\n",
    "    # The difference of this function in comparition with Classification Model for Train data, is this function don't need CATEGORY.\n",
    "    sparse_predict_df = make_sparse_matrix(word_col_df, vocab)\n",
    "    # Grouped by MATERIAL_ID\n",
    "    sparse_predict_df_grouped = sparse_predict_df.groupby(['MATERIAL_ID', 'WORD_ID']).sum()\n",
    "    # Reset it index\n",
    "    sparse_predict_df_grouped = sparse_predict_df_grouped.reset_index()\n",
    "    # Convert it into numpy array.\n",
    "    sparse_predict_data = sparse_predict_df_grouped.to_numpy()\n",
    "    \n",
    "    #Full Matrix\n",
    "    predict_full_feature = make_full_feature(sparse_predict_data, vocab.shape[0])\n",
    "    \n",
    "    #Joint probability in log format\n",
    "    joint_log_ctg_1 = predict_full_feature.dot(np.log(train_data_1) - np.log(train_data_all)) + np.log(prob_ctg_1_train_data)\n",
    "    joint_log_ctg_0 = predict_full_feature.dot(np.log(train_data_0)-np.log(train_data_all))+np.log(1 - prob_ctg_1_train_data)\n",
    "    # Prediction\n",
    "    prediction_log = joint_log_ctg_1 > joint_log_ctg_0\n",
    "    \n",
    "    #Get the index of the row that predicted as material in the bom\n",
    "    row_list = prediction_log[prediction_log == True].index\n",
    "    \n",
    "    # Get the material from the original bom by the index in row_list\n",
    "    material_list = bom.loc[row_list,:]\n",
    "    \n",
    "    return material_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walk through a workbook \n",
    "* Check each sheet in the workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sheet_checkor(BOM, Style_name, SHEET_NAME_LIST = SHEET_NAME_LIST):\n",
    "    xl = pd.ExcelFile(BOM)\n",
    "    \n",
    "    for sheet in xl.sheet_names:\n",
    "        name = str(sheet).lower()\n",
    "        if any(x in name for x in SHEET_NAME_LIST): \n",
    "            bom = xl.parse(sheet, index_col = None, header = None)\n",
    "            bom_list = material_list_generator(bom)\n",
    "            \n",
    "            # Delete empty columns\n",
    "            for col in bom_list:\n",
    "                if bom_list[col].count() == 0:\n",
    "                    bom_list = bom_list.drop(col, axis = 1)\n",
    "\n",
    "            bom_list.to_csv('result/' + Style_name + '_' + sheet + '_material_list.csv')\n",
    "            print(name)\n",
    "            \n",
    "    print('Job finished')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walk through a dir \n",
    "* Check each xlsx in a dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def material_list_convertor(FROM_DIR, TO_DIR):\n",
    "    converted_xlsm_num = 0\n",
    "    for root, dirnames, filenames in walk(FROM_DIR):\n",
    "        # walk through each xlsx file\n",
    "        for file_name in filenames:\n",
    "             # get the path of the file\n",
    "            # Appoint the method only work with .xlsx file.\n",
    "            if file_name.endswith('.xlsx') :\n",
    "                converted_xlsm_num = converted_xlsm_num + 1\n",
    "                filepath = join(root, file_name)\n",
    "                # Custom function\n",
    "                sheet_checkor(filepath, file_name[0:-5])\n",
    "    print('Converted ', converted_xlsm_num, ' files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bom\n",
      "Job finished\n",
      "084g bom\n",
      "Job finished\n",
      "084 bom\n",
      "Job finished\n",
      "081 bom\n",
      "Job finished\n",
      "shell\n",
      "trims\n",
      "Job finished\n",
      "shell\n",
      "trims\n",
      "Job finished\n",
      "032 bom\n",
      "Job finished\n",
      "075 bom\n",
      "Job finished\n",
      "Converted  8  files\n",
      "CPU times: user 4.06 s, sys: 74.1 ms, total: 4.13 s\n",
      "Wall time: 2.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "material_list_convertor(FROM_DIR, TO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
