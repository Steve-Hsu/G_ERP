{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 010_12 - 01_Vocabulary_fixer - Part_1\n",
    "* This func separate in 2 parts\n",
    "    * Part 1, generator a vocabulary with columns of prob_tokens_ctg_1, prob_tokens_ctg_0, and prob_tokens_all, and saved in CSV format\n",
    "    * Part 2, read the vocabulary above and generate new vocabulary, new prob_tokens_ctg_1, and new prob_tokens_ctg_0\n",
    "\n",
    "* Between Part 1 and Part 2 you should do\n",
    "    * Fix the result of part by manually deleting the token you think should not be a token\n",
    "    * The tokens that have ambiguous value between prob_tokens_ctg_1 and prob_tokens_ctg_0 is the one you should consider to delete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Switch area\n",
    "SUBJECT = 'unit'\n",
    "# SUBJECT_set_1 for col classify= [ 'brand', 'style', 'item', 'description', 'spec', 'width:', 'weight:', 'position', 'color_way', 'supplier', 'ref', 'qty', 'price/unit', 'unit', ]\n",
    "\n",
    "VOCAB_SIZE = 900\n",
    "# If VOCAB_SIZE is great than the row of database, it will cause error and stop the calculating on the step \"Create Vocabulary with WORD_ID\".\n",
    "\n",
    "TEST_SIZE = 0.1 # proportion, the size of test_set\n",
    "\n",
    "# for record the result in the excel file.\n",
    "RECORD_FILE = 'Matrix_and_Trained-data_records_main-file.csv'\n",
    "\n",
    "# Remenber to fillout it by manual !!!!!\n",
    "# The matrix you used in the 010_01_Matrix_Generator, for checking the precision of matrix, \n",
    "MATRIX = 'col_classify_matrix_trimed_delete-none-material-col_20200404.csv'\n",
    "# MATRIX = 'col_classify_matrix_trimed_20200330_double_the_True_description.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Generate DF of Vocab with probs in CSV format\n",
    "### Reading trained datas\n",
    "* vocabulary, prob_tokens_ctg_1 and prob_tokens_ctg_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary\n",
    "vocab = pd.read_csv('result/Trained Data/Vocabulary/' + SUBJECT + '_vocabulary.csv', index_col = 0)\n",
    "# Probability of tokens of True in train_set\n",
    "train_data_1 = np.loadtxt('result/Trained Data/Train_set/' + SUBJECT + '_prob_tokens_ctg_1_in_train_data')\n",
    "# Probability of tokens of False in Train_set\n",
    "train_data_0 = np.loadtxt('result/Trained Data/Train_set/' + SUBJECT + '_prob_tokens_ctg_0_in_train_data')\n",
    "# Tokens frequency in Train_set\n",
    "train_data_all = np.loadtxt('result/Trained Data/Train_set/' + SUBJECT + '_prob_tokens_all_in_train_data')\n",
    "\n",
    "# The percentage of True documents in the train_set\n",
    "prob_ctg_1 = pd.read_csv('result/Trained Data/Train_set/' + SUBJECT + '_prob_ctg_1_in_train_data', index_col = 0)\n",
    "prob_ctg_1_train_data = prob_ctg_1.loc[0, 'prob_ctg_1_train_set']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the prob_datas to vocab as columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.insert(1, 'prob_ctg_1', train_data_1, allow_duplicates = False)\n",
    "vocab.insert(2, 'prob_ctg_0', train_data_0, allow_duplicates = False)\n",
    "vocab.insert(3, 'prob_in_all_doc', train_data_all, allow_duplicates = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the new vocabulary\n",
    "* The new vocabuary in csv now is read for manual fix\n",
    "* Open the new vocabulary adjust it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.to_csv('result/manual_fix_vocab/01_For_fix/'+SUBJECT+'_for_mlFix_vocabulary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Go to fixed the Vocabulary right now !!!\n",
    "## . . .\n",
    "## . .\n",
    "## .\n",
    "## Save the manully fixed file in the folder \"02_Fixed_vocab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VOCAB_WORD</th>\n",
       "      <th>prob_ctg_1</th>\n",
       "      <th>prob_ctg_0</th>\n",
       "      <th>prob_in_all_doc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WORD_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>black</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.024175</td>\n",
       "      <td>0.024408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>pocket</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.024986</td>\n",
       "      <td>0.025227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>tape</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.020265</td>\n",
       "      <td>0.020459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>zipper</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.020146</td>\n",
       "      <td>0.020338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>color</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.019645</td>\n",
       "      <td>0.019832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>895</td>\n",
       "      <td>shorter</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>896</td>\n",
       "      <td>jean</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>897</td>\n",
       "      <td>confirm</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>898</td>\n",
       "      <td>blair</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>899</td>\n",
       "      <td>dye</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        VOCAB_WORD  prob_ctg_1  prob_ctg_0  prob_in_all_doc\n",
       "WORD_ID                                                    \n",
       "0            black    0.001044    0.024175         0.024408\n",
       "1           pocket    0.001044    0.024986         0.025227\n",
       "2             tape    0.001044    0.020265         0.020459\n",
       "3           zipper    0.001044    0.020146         0.020338\n",
       "4            color    0.001044    0.019645         0.019832\n",
       "...            ...         ...         ...              ...\n",
       "895        shorter    0.001044    0.000072         0.000060\n",
       "896           jean    0.001044    0.000072         0.000060\n",
       "897        confirm    0.001044    0.000072         0.000060\n",
       "898          blair    0.001044    0.000060         0.000048\n",
       "899            dye    0.001044    0.000072         0.000060\n",
       "\n",
       "[900 rows x 4 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
