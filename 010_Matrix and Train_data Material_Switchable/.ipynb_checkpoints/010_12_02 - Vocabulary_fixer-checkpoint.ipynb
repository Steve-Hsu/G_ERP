{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 010_12 - 02_Vocabulary_fixer\n",
    "* This func separate in 2 parts\n",
    "    * Part 1, generator a vocabulary with columns of prob_tokens_ctg_1, prob_tokens_ctg_0, and prob_tokens_all, and saved in CSV format\n",
    "    * Part 2, read the vocabulary above and generate new vocabulary, new prob_tokens_ctg_1, and new prob_tokens_ctg_0\n",
    "\n",
    "* Between Part 1 and Part 2 you should do\n",
    "    * Fix the result of part by manually deleting the token you think should not be a token\n",
    "    * The tokens that have ambiguous value between prob_tokens_ctg_1 and prob_tokens_ctg_0 is the one you should consider to delete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Switch area\n",
    "SUBJECT = 'color_way'\n",
    "# SUBJECT_set_1 for col classify= [ 'brand', 'style', 'item', 'description', 'spec', 'width:', 'weight:', 'position', 'color_way', 'supplier', 'ref', 'qty', 'price/unit', 'unit', ]\n",
    "\n",
    "VOCAB_SIZE = 900\n",
    "# If VOCAB_SIZE is great than the row of database, it will cause error and stop the calculating on the step \"Create Vocabulary with WORD_ID\".\n",
    "\n",
    "TEST_SIZE = 0.1 # proportion, the size of test_set\n",
    "\n",
    "# for record the result in the excel file.\n",
    "RECORD_FILE = 'Matrix_and_Trained-data_records_main-file.csv'\n",
    "\n",
    "# Remenber to fillout it by manual !!!!!\n",
    "# The matrix you used in the 010_01_Matrix_Generator, for checking the precision of matrix, \n",
    "MATRIX = 'col_classify_matrix_trimed_delete-none-material-col_20200404.csv'\n",
    "# MATRIX = 'col_classify_matrix_trimed_20200330_double_the_True_description.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After fixed the Vocabulary mannually\n",
    "## . . .\n",
    "## . .\n",
    "## ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Generate new datas from the fixed vacabulary\n",
    "* Following codes should generate new \"_prob_tokens_ctg_1_in_train_data\" and \"_prob_tokens_ctg_0_in_train_data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the manually fixed vocabulary\n",
    "* This vocabulary read must be the one that is fixed by me manually deleting the token that has ambiguous result in prob_ctg_1 and prob_ctg_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_vocab = pd.read_csv('result/manual_fix_vocab/02_Fixed_vocab/'+SUBJECT+'_for_mlFix_vocabulary_mlFixed_v1.csv', index_col = 'WORD_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VOCAB_WORD</th>\n",
       "      <th>prob_ctg_1</th>\n",
       "      <th>prob_ctg_0</th>\n",
       "      <th>prob_in_all_doc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WORD_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>black</td>\n",
       "      <td>0.148424</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>0.118005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>pocket</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.036465</td>\n",
       "      <td>0.008596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>tape</td>\n",
       "      <td>0.014454</td>\n",
       "      <td>0.009022</td>\n",
       "      <td>0.015248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>zipper</td>\n",
       "      <td>0.007458</td>\n",
       "      <td>0.018743</td>\n",
       "      <td>0.011367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>label</td>\n",
       "      <td>0.003142</td>\n",
       "      <td>0.025836</td>\n",
       "      <td>0.009195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>895</td>\n",
       "      <td>chic</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.165062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>896</td>\n",
       "      <td>ind</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.267435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>897</td>\n",
       "      <td>led</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.269865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>898</td>\n",
       "      <td>yz</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>899</td>\n",
       "      <td>corey</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        VOCAB_WORD  prob_ctg_1  prob_ctg_0  prob_in_all_doc\n",
       "WORD_ID                                                    \n",
       "0            black    0.148424    0.001097         0.118005\n",
       "1           pocket    0.000066    0.036465         0.008596\n",
       "2             tape    0.014454    0.009022         0.015248\n",
       "3           zipper    0.007458    0.018743         0.011367\n",
       "4            label    0.003142    0.025836         0.009195\n",
       "...            ...         ...         ...              ...\n",
       "895           chic    0.000013    0.000057         0.165062\n",
       "896            ind    0.000013    0.000095         0.267435\n",
       "897            led    0.000013    0.000057         0.269865\n",
       "898             yz    0.000013    0.000095         0.500000\n",
       "899          corey    0.000013    0.000095         1.000000\n",
       "\n",
       "[900 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the final result of probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the new vocabulary\n",
    "new_vocab = pd.DataFrame(fixed_vocab['VOCAB_WORD'])\n",
    "new_vocab.to_csv('result/manual_fix_vocab/03_Fixed_result/' + SUBJECT + '_mlFixed_vocabulary.csv')\n",
    "\n",
    "# Save the prob_ctg_1\n",
    "np.savetxt('result/manual_fix_vocab/03_Fixed_result/' + SUBJECT + '_mlFixed_prob_tokens_ctg_1_in_train_data', fixed_vocab['prob_ctg_1'])\n",
    "# Save the prob_ctg_0\n",
    "np.savetxt('result/manual_fix_vocab/03_Fixed_result/' + SUBJECT + '_mlFixed_prob_tokens_ctg_0_in_train_data', fixed_vocab['prob_ctg_0'])\n",
    "# Save the prob_ctg_all\n",
    "np.savetxt('result/manual_fix_vocab/03_Fixed_result/' + SUBJECT + '_mlFixed_prob_tokens_all_in_train_data', fixed_vocab['prob_ctg_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
