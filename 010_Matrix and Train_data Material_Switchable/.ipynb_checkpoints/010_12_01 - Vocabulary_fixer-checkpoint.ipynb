{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 010_12 - 01_Vocabulary_fixer - Part_1\n",
    "* This func separate in 2 parts\n",
    "    * Part 1, generator a vocabulary with columns of prob_tokens_ctg_1, prob_tokens_ctg_0, and prob_tokens_all, and saved in CSV format\n",
    "    * Part 2, read the vocabulary above and generate new vocabulary, new prob_tokens_ctg_1, and new prob_tokens_ctg_0\n",
    "\n",
    "* Between Part 1 and Part 2 you should do\n",
    "    * Fix the result of part by manually deleting the token you think should not be a token\n",
    "    * The tokens that have ambiguous value between prob_tokens_ctg_1 and prob_tokens_ctg_0 is the one you should consider to delete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Switch area\n",
    "SUBJECT = 'color_way'\n",
    "# SUBJECT_set_1 for col classify= [ 'brand', 'style', 'item', 'description', 'spec', 'width:', 'weight:', 'position', 'color_way', 'supplier', 'ref', 'qty', 'price/unit', 'unit', ]\n",
    "\n",
    "VOCAB_SIZE = 900\n",
    "# If VOCAB_SIZE is great than the row of database, it will cause error and stop the calculating on the step \"Create Vocabulary with WORD_ID\".\n",
    "\n",
    "TEST_SIZE = 0.1 # proportion, the size of test_set\n",
    "\n",
    "# for record the result in the excel file.\n",
    "RECORD_FILE = 'Matrix_and_Trained-data_records_main-file.csv'\n",
    "\n",
    "# Remenber to fillout it by manual !!!!!\n",
    "# The matrix you used in the 010_01_Matrix_Generator, for checking the precision of matrix, \n",
    "MATRIX = 'col_classify_matrix_trimed_delete-none-material-col_20200404.csv'\n",
    "# MATRIX = 'col_classify_matrix_trimed_20200330_double_the_True_description.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Generate DF of Vocab with probs in CSV format\n",
    "### Reading trained datas\n",
    "* vocabulary, prob_tokens_ctg_1 and prob_tokens_ctg_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary\n",
    "vocab = pd.read_csv('result/Trained Data/Vocabulary/' + SUBJECT + '_vocabulary.csv', index_col = 0)\n",
    "# Probability of tokens of True in train_set\n",
    "train_data_1 = np.loadtxt('result/Trained Data/Train_set/' + SUBJECT + '_prob_tokens_ctg_1_in_train_data')\n",
    "# Probability of tokens of False in Train_set\n",
    "train_data_0 = np.loadtxt('result/Trained Data/Train_set/' + SUBJECT + '_prob_tokens_ctg_0_in_train_data')\n",
    "# Tokens frequency in Train_set\n",
    "train_data_all = np.loadtxt('result/Trained Data/Train_set/' + SUBJECT + '_prob_tokens_all_in_train_data')\n",
    "\n",
    "# The percentage of True documents in the train_set\n",
    "prob_ctg_1 = pd.read_csv('result/Trained Data/Train_set/' + SUBJECT + '_prob_ctg_1_in_train_data', index_col = 0)\n",
    "prob_ctg_1_train_data = prob_ctg_1.loc[0, 'prob_ctg_1_train_set']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the prob_datas to vocab as columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.insert(1, 'prob_ctg_1', train_data_1, allow_duplicates = False)\n",
    "vocab.insert(2, 'prob_ctg_0', train_data_0, allow_duplicates = False)\n",
    "vocab.insert(3, 'prob_in_all_doc', train_data_all, allow_duplicates = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the new vocabulary\n",
    "* The new vocabuary in csv now is read for manual fix\n",
    "* Open the new vocabulary adjust it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.to_csv('result/manual_fix_vocab/01_For_fix/'+SUBJECT+'_for_mlFix_vocabulary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Go to fixed the Vocabulary right now !!!\n",
    "## . . .\n",
    "## . .\n",
    "## .\n",
    "## Save the manully fixed file in the folder \"02_Fixed_vocab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
