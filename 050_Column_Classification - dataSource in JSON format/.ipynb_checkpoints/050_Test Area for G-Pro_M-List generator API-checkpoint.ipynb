{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100 % same as the \"mListFuncs.py\" in Project G-Pro_M-List generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Switch area\n",
    "VOCAB_SIZE = 900\n",
    "CATEGORY_LIST = ['fabric', 'insulation', 'interfacing', 'zipper', 'label']\n",
    "COL_LIST = ['item', 'description', 'color_way', 'position', 'spec']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete_col()\n",
    "### Delete the none columns in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_col(m_list):\n",
    "    for col in m_list:\n",
    "        if m_list[col].count() == 0:\n",
    "            m_list = m_list.drop(col, axis = 1)\n",
    "    return m_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stemmered_nltk_convert()\n",
    "###  Nltk stemmered Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmered_nltk_convert(col_of_df):\n",
    "    '''\n",
    "    Parameter of this function is a column of a dataFrame.\n",
    "\n",
    "    '''\n",
    "    # difine Stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Difine Stemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    "\n",
    "    # converts to lower case and splits up the words\n",
    "    words = word_tokenize(col_of_df)\n",
    "    filtered_words = []\n",
    "\n",
    "    for word in words:\n",
    "        # Removes the stop words and punctuation\n",
    "        # if word is not in the stop_words list and is not a alpha.\n",
    "        if word not in stop_words and word.isalpha():\n",
    "            filtered_words.append(stemmer.stem(word))\n",
    "\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## turn_row_series()\n",
    "### Walk through bom\n",
    "### Copyed from 020_02_01 - M_list, original func name is turn_series()\n",
    "* Parse a xlsm of bom, turn the row into a cell, all the cells will form a col.\n",
    "* Put the cell to the classify function\n",
    "* Return the index of row that is True\n",
    "* Used in material-classify, M-List generator, since it try to classify material, that in a sheet is, usually entered as a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_row_series(bom):\n",
    "    '''\n",
    "\n",
    "    The parameter of the func is a dataFrame\n",
    "\n",
    "    '''\n",
    "    database = []\n",
    "\n",
    "    for row in bom.index:\n",
    "        row_str = str()\n",
    "        for col in bom:\n",
    "            row_str = row_str + ', ' + str(bom.at[row, col])\n",
    "        database.append(row_str)\n",
    "\n",
    "    col = pd.Series(database)\n",
    "#     index_list = classify_series(col)\n",
    "\n",
    "    return col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## turn_col_series()\n",
    "### Walk through bom\n",
    "### Copyed from 040_21 - Column-Classify\n",
    "* Parse a xlsm of bom, turn the columns into cells, all the cells will form a col.\n",
    "* Put the cell to the classify function\n",
    "* Return the index of column that is True\n",
    "* Used in column classify, since the information such as item, position, ref_no, supplier, etc, usually are entered as a column in bom sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_col_series(bom):\n",
    "    '''\n",
    "    bom: DataFrame,\n",
    "\n",
    "    '''\n",
    "    database = []\n",
    "\n",
    "    for col in bom:\n",
    "        col_str = str()\n",
    "        for row in bom.index:\n",
    "            col_str = col_str + ', ' + str(bom.at[row, col])\n",
    "        database.append(col_str)\n",
    "\n",
    "    col = pd.Series(database)\n",
    "#     index_list = classify_series(col)\n",
    "\n",
    "    return col  # Below is the original function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make_sparse_matrix()\n",
    "### Sparse Matrix Function\n",
    "* Create a sparse Matrix for the data we want to predict\n",
    "* The difference of this function in comparition with Classification Model for Train data, is this function don't need CATEGORY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sparse_matrix(df, vocabulary):\n",
    "    \"\"\"\n",
    "    Param1:\n",
    "    The data we want to sparse, which must be in format of DataFrame.\n",
    "    Param2:\n",
    "    The vocabulary, it is generated when we training datas.\n",
    "\n",
    "    Returns a sparse matrix as dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    indexed_words = pd.Index(vocabulary.VOCAB_WORD)\n",
    "    nr_rows = df.shape[0]\n",
    "    nr_cols = df.shape[1]\n",
    "    word_set = set(indexed_words)\n",
    "    dict_list = []\n",
    "\n",
    "    for i in range(nr_rows):\n",
    "        for j in range(nr_cols):\n",
    "\n",
    "            word = df.iat[i, j]\n",
    "            if word in word_set:\n",
    "                doc_id = df.index[i]\n",
    "                word_id = indexed_words.get_loc(word)\n",
    "\n",
    "                item = {'MATERIAL_ID': doc_id,\n",
    "                        'OCCURENCE': 1, 'WORD_ID': word_id}\n",
    "\n",
    "                dict_list.append(item)\n",
    "\n",
    "    return pd.DataFrame(dict_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make_full_feature()\n",
    "## Full Matrix\n",
    "* Since we want to predict the data, so we create the Full Feature directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_full_feature(sparse_matrix, nr_words, doc_idx=0, word_idx=1, freq_idx=2):\n",
    "    column_names = ['MATERIAL_ID'] + list(range(0, VOCAB_SIZE))\n",
    "    doc_id_names = np.unique(sparse_matrix[:, 0])\n",
    "    full_matrix = pd.DataFrame(index=doc_id_names, columns=column_names)\n",
    "    full_matrix.fillna(value=0, inplace=True)\n",
    "\n",
    "    for i in range(sparse_matrix.shape[0]):\n",
    "        doc_nr = sparse_matrix[i][doc_idx]\n",
    "        word_id = sparse_matrix[i][word_idx]\n",
    "        occurrence = sparse_matrix[i][freq_idx]\n",
    "\n",
    "        full_matrix.at[doc_nr, 'MATERIAL_ID'] = doc_nr\n",
    "        full_matrix.at[doc_nr, word_id] = occurrence\n",
    "\n",
    "    full_matrix.set_index('MATERIAL_ID', inplace=True)\n",
    "    return full_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Funciton -----------------------------------------------------\n",
    "* The func in this area is not root func. The func here uses some other func as part of it.\n",
    "\n",
    "## Part -1\n",
    "## classifer()\n",
    "* It merged, material_classfor() from 20, 30, and 40.\n",
    "* It could classify in row, to be a M-List or classifying category of materials, or in column, to classify columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifer(bom, switcher, SUBJECT):\n",
    "    # DATA_SOURCE = ''\n",
    "    if switcher == 1:\n",
    "        DATA_SOURCE = 'DataSource/Trained-Data_M-List/'\n",
    "        SUBJECT = 'material_garment'\n",
    "    elif switcher == 2:\n",
    "        DATA_SOURCE = 'DataSource/Trained-Data_ROW-classify/'\n",
    "    elif switcher == 3:\n",
    "        DATA_SOURCE = 'DataSource/Trained-Data_COL-classify/'\n",
    "    VOCAB = DATA_SOURCE + SUBJECT + '_vocabulary.csv'\n",
    "    TRAIN_DATA_1 = DATA_SOURCE + SUBJECT + '_prob_tokens_ctg_1_in_train_data'\n",
    "    TRAIN_DATA_0 = DATA_SOURCE + SUBJECT + '_prob_tokens_ctg_0_in_train_data'\n",
    "    TRAIN_DATA_ALL = DATA_SOURCE + SUBJECT + '_prob_tokens_all_in_train_data'\n",
    "    PROB_1_TRAIN_DATA = DATA_SOURCE + SUBJECT + '_prob_ctg_1_in_train_data'\n",
    "    '''\n",
    "    Param_1\n",
    "    Bom : DataFrame, bom turned into dataFrame before input to this function\n",
    "\n",
    "    Parame_2\n",
    "    Switcher : Int, Define what list your want to make.\n",
    "    1 = Step_1 = M_List : turn a bom into M_List\n",
    "    2 = Step_2 = Cagetory classify : classify the category of each material, usually in row, \n",
    "    3 = Step_3 = Column classify : classify the column name, usually in column\n",
    "\n",
    "    Param_3\n",
    "    SUBJECT : String, decide whitch trained_data to use.\n",
    "\n",
    "    Param_4\n",
    "    The path of the vocabulary\n",
    "    Token list with WORD_ID\n",
    "\n",
    "    Param_5\n",
    "    The trained data of catagory True\n",
    "    Probabilitie of each token in category True\n",
    "\n",
    "    Param_6\n",
    "    The trained data of category False\n",
    "    Probabilitie of each token in category False\n",
    "\n",
    "    Param_7\n",
    "    The trained data of category both.\n",
    "    Probabilitie of each token in all documents\n",
    "\n",
    "    Param_8\n",
    "    The percentage of documents in catagory True in all documents.\n",
    "    Number of documents in catagory True / number of all documents\n",
    "    '''\n",
    "    # read the vocabulary\n",
    "    vocab = pd.read_csv(VOCAB, index_col=0)\n",
    "    # read the trained_datas\n",
    "    train_data_1 = np.loadtxt(TRAIN_DATA_1)\n",
    "    train_data_0 = np.loadtxt(TRAIN_DATA_0)\n",
    "    train_data_all = np.loadtxt(TRAIN_DATA_ALL)\n",
    "    prob_ctg_1 = pd.read_csv(PROB_1_TRAIN_DATA, index_col=0)\n",
    "    prob_ctg_1_train_data = prob_ctg_1.loc[0, 'prob_ctg_1_train_set']\n",
    "\n",
    "    # Read the file\n",
    "    col_deleted_bom = delete_col(bom)\n",
    "\n",
    "    # Switcher\n",
    "    if switcher < 3:\n",
    "        # For M_List of material classify, Parse the bom, make each cell in a row getting together to be 1 cell\n",
    "        new_bom = turn_row_series(col_deleted_bom)\n",
    "    else:\n",
    "        # For column classify, Parse the bom, make each col getting together to be 1 cell\n",
    "        new_bom = turn_col_series(col_deleted_bom)\n",
    "\n",
    "    # nltk_convert\n",
    "    stemmed_bom = new_bom.apply(stemmered_nltk_convert)\n",
    "\n",
    "    # Convert the stemmed series into df\n",
    "    # 1 token get 1 cell\n",
    "    word_col_df = pd.DataFrame.from_records(stemmed_bom.tolist())\n",
    "\n",
    "    # Sparse Matrix\n",
    "    # Create a sparse Matrix for the data we want to predict\n",
    "    # The difference of this function in comparition with Classification Model for Train data, is this function don't need CATEGORY.\n",
    "    sparse_predict_df = make_sparse_matrix(word_col_df, vocab)\n",
    "    # Grouped by MATERIAL_ID\n",
    "    sparse_predict_df_grouped = sparse_predict_df.groupby(\n",
    "        ['MATERIAL_ID', 'WORD_ID']).sum()\n",
    "    # Reset it index\n",
    "    sparse_predict_df_grouped = sparse_predict_df_grouped.reset_index()\n",
    "    # Convert it into numpy array.\n",
    "    sparse_predict_data = sparse_predict_df_grouped.to_numpy()\n",
    "\n",
    "    # Full Matrix\n",
    "    predict_full_feature = make_full_feature(\n",
    "        sparse_predict_data, vocab.shape[0])\n",
    "\n",
    "    # Joint probability in log format\n",
    "    joint_log_ctg_1 = predict_full_feature.dot(\n",
    "        np.log(train_data_1) - np.log(train_data_all)) + np.log(prob_ctg_1_train_data)\n",
    "    joint_log_ctg_0 = predict_full_feature.dot(\n",
    "        np.log(train_data_0)-np.log(train_data_all))+np.log(1 - prob_ctg_1_train_data)\n",
    "    # Prediction\n",
    "    prediction_log = joint_log_ctg_1 > joint_log_ctg_0\n",
    "\n",
    "    # Get the index of the row that predicted as material in the bom\n",
    "    row_list = prediction_log[prediction_log == True].index\n",
    "\n",
    "    if switcher == 1:\n",
    "        # For M_List, the job is finished, return it and save it as a CSV\n",
    "        material_list = bom.loc[row_list, :]\n",
    "        material_list.to_csv('result/M-List.csv')\n",
    "        return material_list\n",
    "    else:\n",
    "        # For material category or column classify, return the index, for next function to finish it job\n",
    "        return row_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Material classify Part-2\n",
    " * Analyze a M-List with several sets of train-data and vocabulary.\n",
    " * Each set of trainned-data and vocabulary represents 1 category of material, such as fabric, zipper, label.\n",
    " * This func will feed the func \"material_classifor\" each set of trainned-data and vocabulary by order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_category(bomPath, Style_name, LIST=CATEGORY_LIST):\n",
    "    '''\n",
    "    Param_1\n",
    "    bomPath : Path to M_list in CSV format\n",
    "\n",
    "    Param_2\n",
    "    style_name: String\n",
    "    The name of the file.\n",
    "\n",
    "    Param_3\n",
    "    An array in type List.\n",
    "    The content is the categories of material.\n",
    "    That decides which train_set data to be used.\n",
    "    '''\n",
    "    # Set index_col=None, In this part we have to make new index for each row.\n",
    "    bom = pd.read_csv(bomPath, index_col=None, encoding='ISO-8859-1')\n",
    "    bom.insert(1, 'CATEGORY', 'other', True)\n",
    "\n",
    "    for cate in LIST:\n",
    "        ROW_LIST = classifer(bom, 2, cate)\n",
    "        bom.at[ROW_LIST, 'CATEGORY'] = cate\n",
    "\n",
    "    bom.to_csv('result/' + Style_name + '_ROW_classified_M-List.csv')\n",
    "    return bom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-3\n",
    "## From 040_21 - Column_Classify\n",
    "## Loop through materials designated as category.\n",
    "* Analyze a Categoried_M-List with several sets of train-data and vocabulary.\n",
    "* Each set of trainned-data and vocabulary represents 1 category of material, such as item, description, spec.\n",
    "* This func will feed the func \"material_classifor\" each set of trainned-data and vocabulary by order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_category(M_list, Style_name, LIST=COL_LIST):\n",
    "    '''\n",
    "    # Arguments\n",
    "    Param_1\n",
    "    bomPath : Path to M_list in CSV format\n",
    "\n",
    "    Param_2\n",
    "    Style_name: String,  \n",
    "    the name of the file.\n",
    "\n",
    "    Param_3\n",
    "    LIST: List,  \n",
    "    An array in type List. The content is the categories of material. That decides which train_set data to be used.\n",
    "\n",
    "    # Returns\n",
    "        A M_List in csv fomat, filled out the name of columns, such as 'item', 'description', 'spec', etc.\n",
    "    '''\n",
    "    # Set index_col=0, Since in the last part, we've created the index, here set the column with index 0 as the index here.\n",
    "    bom = pd.read_csv(M_list, index_col=0, encoding='ISO-8859-1')\n",
    "\n",
    "    # Add a new row\n",
    "    new_bom = bom.append(pd.Series(name='TITLE'))\n",
    "\n",
    "    new_bom.loc['TITLE', 'MATERIAL_ID'] = 'MATERIAL_ID'\n",
    "    new_bom.loc['TITLE', 'CATEGORY'] = 'CATEGORY'\n",
    "\n",
    "    for cate in LIST:\n",
    "        ROW_LIST = classifer(bom, 3, cate)\n",
    "        print(ROW_LIST.values)\n",
    "\n",
    "        # Loop through the ROW_LIST, change the cell in the row \"TITLE\" and in the col that with integer postion as the Int in ROW_LIST.\n",
    "        # notice, the method iat can only handle one col for one time, so here must use For Loop to get the job done.\n",
    "        # Since the new row \"TITLE\" is on the buttom of the df, the new_bom, so the integer position of the row is '-1'\n",
    "        num = 0\n",
    "        for index in ROW_LIST:\n",
    "            # since papa parse will ignore the duplicated columan, so we add index to make each column name uniqle, even when it in same name like \"color_way\"\n",
    "            num = num + 1\n",
    "            if cate == 'color_way':  # Because my model and trained data is named 'color_way'\n",
    "                new_bom.iat[-1, index] = 'colorway_' + str(num)\n",
    "            else:\n",
    "                new_bom.iat[-1, index] = cate\n",
    "\n",
    "    # # Update the empty Title as undefined\n",
    "    # Since the type in numpy.float64 can not be rewrite with str, we have to cast it into float, which can keep the empty cell as empty, not return \"nan\", also can be rewrite by str\n",
    "\n",
    "    num = 1\n",
    "    for i in range(len(new_bom.iloc[-1])):\n",
    "        # print(new_bom.iat[-1, i]) # Test Code\n",
    "        print(\"the type\", type(new_bom.iat[-1, i]))  # Test Code\n",
    "        if(type(new_bom.iat[-1, i]) != str):\n",
    "            if(type(new_bom.iat[-1, i]) == float):\n",
    "                print(\"convert is triggered\")\n",
    "                new_bom.iat[-1, i] = str(\"undefined_\" + str(num))\n",
    "                num = num + 1\n",
    "\n",
    "    # Set the row \"TITLE\" as the columns of the new_bom\n",
    "    new_bom.columns = new_bom.iloc[-1]\n",
    "\n",
    "    # # Delete the last row, that must and should be the row \"TITLE\"\n",
    "    new_bom.drop(new_bom.index[-1], inplace=True)\n",
    "\n",
    "    # Why here reset_index is becasue the papa-parse of javascript will take the 1st row as the header but not the column label in the dataframe\n",
    "    # The column label in the dataFrame is the name of each column, but with no index, the index start from the 1st row as 0, but not the column of label.\n",
    "    # So in here I have to reset the index again, make the index 0 start from the row, witch is inserted as row \"TITLE\", so when papa-parse to parse it, it will take the correct row as the header for each row\n",
    "    new_bom.reset_index\n",
    "    new_bom.to_csv('result/' + Style_name + '_COL_ROW_classified_M-List.csv')\n",
    "    return new_bom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test area\n",
    "* Test the functions above, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walk through a dir - Not fixing yet !!!\n",
    "* Check each JSON in a dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_category_walkthrough_dir(FROM_DIR, TO_DIR):\n",
    "    converted_csv_num = 0\n",
    "    for root, dirnames, filenames in walk(FROM_DIR):\n",
    "        # walk through each xlsx file\n",
    "        for file_name in filenames:\n",
    "             # get the path of the file\n",
    "            # Appoint the method only work with .xlsx file.\n",
    "            if file_name.endswith('.JSON') :\n",
    "                converted_csv_num = converted_csv_num + 1\n",
    "                filepath = join(root, file_name)\n",
    "                # Custom function\n",
    "                col_category(filepath, file_name[0:-18])\n",
    "    print('Converted ', converted_csv_num, ' files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
